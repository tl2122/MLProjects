{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Install packages"
      ],
      "metadata": {
        "id": "zOfNFg7FQ-hV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwbDJE2XRAWM",
        "outputId": "59a020c1-9d3f-43f7-c1ff-a87816b63059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "(Reading database ... 155569 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u342-b07-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u342-b07-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 41 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 54.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=4f4432941bf6dfceb664663b2c1290620e6345f873538e7f2d2a39bbc193c4ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n",
            "--2022-09-21 08:41:09--  https://apache.osuosl.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz\n",
            "Resolving apache.osuosl.org (apache.osuosl.org)... 64.50.233.100, 64.50.236.52, 140.211.166.134, ...\n",
            "Connecting to apache.osuosl.org (apache.osuosl.org)|64.50.233.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 299321244 (285M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.3.0-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.3.0-bin-had 100%[===================>] 285.45M  86.7MB/s    in 4.6s    \n",
            "\n",
            "2022-09-21 08:41:14 (61.7 MB/s) - ‘spark-3.3.0-bin-hadoop3.tgz’ saved [299321244/299321244]\n",
            "\n",
            "spark-3.3.0-bin-hadoop3.tgz\n",
            "\n",
            "spark-3.3.0-bin-hadoop3:\n",
            "bin   data\tjars\t    LICENSE   NOTICE  R\t\t RELEASE  yarn\n",
            "conf  examples\tkubernetes  licenses  python  README.md  sbin\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq\n",
        "!pip install findspark\n",
        "!pip install pyspark\n",
        "!wget -v https://apache.osuosl.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz \n",
        "!tar xf spark-3.3.0-bin-hadoop3.tgz\n",
        "!ls sp*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Set up Python environment"
      ],
      "metadata": {
        "id": "iDKU-O7RRPMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.0-bin-hadoop3\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "import pyspark\n",
        "print(pyspark.__version__)\n",
        "!java --version"
      ],
      "metadata": {
        "id": "28tlszjTdsN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "225997a0-f1a9-402b-b991-17420a1a4aaa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.3.0\n",
            "openjdk 11.0.16 2022-07-19\n",
            "OpenJDK Runtime Environment (build 11.0.16+8-post-Ubuntu-0ubuntu118.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.16+8-post-Ubuntu-0ubuntu118.04, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nHeuAFKGyMO",
        "outputId": "fd96fdd1-393f-4782-ffb8-62adab94b387"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy  as np\n",
        "infolder = \"MyDrive/MLData/spark/ml-100k\""
      ],
      "metadata": {
        "id": "Fk1KNPChMpRT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "import collections\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"SparkTest\")\n",
        "sc = SparkContext(conf = conf)"
      ],
      "metadata": {
        "id": "KK0K0ITCM_ON"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with RDD"
      ],
      "metadata": {
        "id": "Be25MhAPRad2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Calculate Average Friends by age"
      ],
      "metadata": {
        "id": "wUgnWB-3S1ze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Friends By Age\n",
        "##------------------------------------------------------------##\n",
        "#conf2 = SparkConf().setMaster(\"local\").setAppName(\"FriendsByAge\")\n",
        "#sc = SparkContext(conf = conf2)\n",
        "\n",
        "def parseLine(line):\n",
        "    fields = line.split(',')\n",
        "    age = int(fields[2])\n",
        "    numFriends = int(fields[3])\n",
        "    return (age, numFriends)\n",
        "\n",
        "lines = sc.textFile(infolder+\"/../fakefriends.csv\")\n",
        "rdd = lines.map(parseLine)\n",
        "totalsByAge = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
        "averagesByAge = totalsByAge.mapValues(lambda x: x[0] / x[1])\n",
        "results = sorted(averagesByAge.collect())\n",
        "for result in results:\n",
        "    if result[0]<30:\n",
        "      print(\"{:6.0f} {:6.2f}\".format(result[0],result[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTLPKnHlHsyM",
        "outputId": "eeae59a2-7884-4928-d420-6cf6e005435b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    18 343.38\n",
            "    19 213.27\n",
            "    20 165.00\n",
            "    21 350.88\n",
            "    22 206.43\n",
            "    23 246.30\n",
            "    24 233.80\n",
            "    25 197.45\n",
            "    26 242.06\n",
            "    27 228.12\n",
            "    28 209.10\n",
            "    29 215.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Total/Sum expenditure by customer "
      ],
      "metadata": {
        "id": "sOKjYcZySfs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extractCustomerPricePairs(line):\n",
        "    fields = line.split(',')\n",
        "    return (int(fields[0]), float(fields[2]))\n",
        "\n",
        "def res_key (res):\n",
        "  return res[1]\n",
        "\n",
        "input = sc.textFile(infolder+\"/../customer-orders.csv\")\n",
        "mappedInput = input.map(extractCustomerPricePairs)\n",
        "totalByCustomer = mappedInput.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "results = totalByCustomer.collect();\n",
        "\n",
        "def res_key (res):\n",
        "  return res[1]\n",
        "results = sorted(results,key=res_key,reverse=True)\n",
        "for result in results:\n",
        "    if (result[1]>5800 or result[1]<3500):\n",
        "      print (\"{:6.0f} {:6.2f}\".format(result[0],result[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmUvozQMMyhx",
        "outputId": "360dfe73-66eb-4b10-e9e1-9266320f2cf8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    68 6375.45\n",
            "    73 6206.20\n",
            "    39 6193.11\n",
            "    54 6065.39\n",
            "    71 5995.66\n",
            "     2 5994.59\n",
            "    97 5977.19\n",
            "    46 5963.11\n",
            "    45 3309.38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Movie Database\n"
      ],
      "metadata": {
        "id": "O2uprvftS744"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lines   = sc.textFile(infolder+\"/u.data\")\n",
        "ratings = lines.map(lambda x: x.split()[2])\n",
        "result  = ratings.countByValue()\n",
        "\n",
        "sortedResults = collections.OrderedDict(sorted(result.items()))\n",
        "for key, value in sortedResults.items():\n",
        "    print(\"%s %i\" % (key, value))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNs4BO1rNIFw",
        "outputId": "f7da0972-92ff-43d6-a653-3fedc604f331"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 6110\n",
            "2 11370\n",
            "3 27145\n",
            "4 34174\n",
            "5 21201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as func\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, LongType\n",
        "import codecs\n",
        "\n",
        "def loadMovieNames():\n",
        "    movieNames = {}\n",
        "    with codecs.open(infolder+\"/u.item\", \"r\", encoding='ISO-8859-1', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            fields = line.split('|')\n",
        "            movieNames[int(fields[0])] = fields[1]\n",
        "    return movieNames"
      ],
      "metadata": {
        "id": "u4jcnUJCWUOS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Calculate highest rating movie"
      ],
      "metadata": {
        "id": "JTk_EaDyTRXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"PopularMovies\").getOrCreate()\n",
        "nameDict = spark.sparkContext.broadcast(loadMovieNames())\n",
        "\n",
        "# Create schema when reading u.data\n",
        "schema = StructType([ \\\n",
        "                     StructField(\"userID\", IntegerType(), True), \\\n",
        "                     StructField(\"movieID\", IntegerType(), True), \\\n",
        "                     StructField(\"rating\", IntegerType(), True), \\\n",
        "                     StructField(\"timestamp\", LongType(), True)])\n",
        "# Load up movie data as dataframe\n",
        "moviesDF    = spark.read.option(\"sep\", \"\\t\").schema(schema).csv(infolder+\"/u.data\")\n",
        "movieCounts = moviesDF.groupBy(\"movieID\").count()\n",
        "# Create a user-defined function to look up movie names from our broadcasted dictionary\n",
        "def lookupName(movieID):\n",
        "    return nameDict.value[movieID]\n",
        "lookupNameUDF = func.udf(lookupName)\n",
        "\n",
        "# Add a movieTitle column using our new udf\n",
        "moviesWithNames = movieCounts.withColumn(\"movieTitle\", lookupNameUDF(func.col(\"movieID\")))\n",
        "# Sort the results\n",
        "sortedMoviesWithNames = moviesWithNames.orderBy(func.desc(\"count\"))\n",
        "# Grab the top 10\n",
        "sortedMoviesWithNames.show(10, False)\n",
        "# Stop the session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1-4-iIShgMg",
        "outputId": "9a175aa3-9516-49bd-ab20-3f9772f050e8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+-----------------------------+\n",
            "|movieID|count|movieTitle                   |\n",
            "+-------+-----+-----------------------------+\n",
            "|50     |583  |Star Wars (1977)             |\n",
            "|258    |509  |Contact (1997)               |\n",
            "|100    |508  |Fargo (1996)                 |\n",
            "|181    |507  |Return of the Jedi (1983)    |\n",
            "|294    |485  |Liar Liar (1997)             |\n",
            "|286    |481  |English Patient, The (1996)  |\n",
            "|288    |478  |Scream (1996)                |\n",
            "|1      |452  |Toy Story (1995)             |\n",
            "|300    |431  |Air Force One (1997)         |\n",
            "|121    |429  |Independence Day (ID4) (1996)|\n",
            "+-------+-----+-----------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Most connected superhero in the movies"
      ],
      "metadata": {
        "id": "gfsQcoisThSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as func\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"MostPopularSuperhero\").getOrCreate()\n",
        "\n",
        "schema = StructType([ \\\n",
        "                     StructField(\"id\", IntegerType(), True), \\\n",
        "                     StructField(\"name\", StringType(), True)])\n",
        "\n",
        "names = spark.read.schema(schema).option(\"sep\", \" \").csv(infolder+\"/../MarvelNames.txt\")\n",
        "lines = spark.read.text(infolder+\"/../MarvelGraph.txt\")\n",
        "\n",
        "# Small tweak vs. what's shown in the video: we trim each line of whitespace as that could\n",
        "# thr off the counts.\n",
        "connections = lines.withColumn(\"id\", func.split(func.trim(func.col(\"value\")), \" \")[0]) \\\n",
        "              .withColumn(\"connections\", func.size(func.split(func.trim(func.col(\"value\")), \" \")) - 1) \\\n",
        "              .groupBy(\"id\").agg(func.sum(\"connections\").alias(\"connections\"))\n",
        "    \n",
        "mostPopular     = connections.sort(func.col(\"connections\").desc()).first()\n",
        "mostPopularName = names.filter(func.col(\"id\") == mostPopular[0]).select(\"name\").first()\n",
        "print(mostPopularName[0] + \" is the most popular superhero with \" + str(mostPopular[1]) + \" co-appearances.\")\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id6Us4-JkxJk",
        "outputId": "3c1721f5-db12-464a-e645-927cb3e687c5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CAPTAIN AMERICA is the most popular superhero with 1933 co-appearances.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Least connected superheros in the movies"
      ],
      "metadata": {
        "id": "-WIpwhl2TuZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as func\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"MostObscureSuperheroes\").getOrCreate()\n",
        "\n",
        "schema = StructType([ \\\n",
        "                     StructField(\"id\", IntegerType(), True), \\\n",
        "                     StructField(\"name\", StringType(), True)])\n",
        "\n",
        "names = spark.read.schema(schema).option(\"sep\", \" \").csv(infolder+\"/../MarvelNames.txt\")\n",
        "lines = spark.read.text(infolder+\"/../MarvelGraph.txt\")\n",
        "\n",
        "# Small tweak vs. what's shown in the video: we trim whitespace from each line as this\n",
        "# could throw the counts off by one.\n",
        "connections = lines.withColumn(\"id\", func.split(func.trim(func.col(\"value\")), \" \")[0]) \\\n",
        "              .withColumn(\"connections\", func.size(func.split(func.trim(func.col(\"value\")), \" \")) - 1) \\\n",
        "              .groupBy(\"id\").agg(func.sum(\"connections\").alias(\"connections\"))\n",
        "    \n",
        "minConnectionCount = connections.agg(func.min(\"connections\")).first()[0]\n",
        "minConnections     = connections.filter(func.col(\"connections\") == minConnectionCount)\n",
        "minConnectionsWithNames = minConnections.join(names, \"id\")\n",
        "\n",
        "print(\"The following characters have only \" + str(minConnectionCount) + \" connection(s):\")\n",
        "\n",
        "minConnectionsWithNames.select(\"name\").show()\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDhudsK3n1WL",
        "outputId": "726095c5-8a6a-4062-ee0f-81a31796a58e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following characters have only 0 connection(s):\n",
            "+--------------------+\n",
            "|                name|\n",
            "+--------------------+\n",
            "|        BERSERKER II|\n",
            "|              BLARE/|\n",
            "|MARVEL BOY II/MARTIN|\n",
            "|MARVEL BOY/MARTIN BU|\n",
            "|      GIURESCU, RADU|\n",
            "|       CLUMSY FOULUP|\n",
            "|              FENRIS|\n",
            "|              RANDAK|\n",
            "|           SHARKSKIN|\n",
            "|     CALLAHAN, DANNY|\n",
            "|         DEATHCHARGE|\n",
            "|                RUNE|\n",
            "|         SEA LEOPARD|\n",
            "|         RED WOLF II|\n",
            "|              ZANTOR|\n",
            "|JOHNSON, LYNDON BAIN|\n",
            "|          LUNATIK II|\n",
            "|                KULL|\n",
            "|GERVASE, LADY ALYSSA|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ML --- Linear Regression using Spark"
      ],
      "metadata": {
        "id": "GgG5zE0OQrRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##ML\n",
        "from __future__ import print_function\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "# Create a SparkSession (Note, the config section is only for Windows!)\n",
        "spark = SparkSession.builder.appName(\"LinearRegression\").getOrCreate()\n",
        "\n",
        "# Load up our data and convert it to the format MLLib expects.\n",
        "inputLines = spark.sparkContext.textFile(infolder+\"/../regression.txt\")\n",
        "data = inputLines.map(lambda x: x.split(\",\")).map(lambda x: (float(x[0]), Vectors.dense(float(x[1]))))\n",
        "\n",
        "# Convert this RDD to a DataFrame\n",
        "colNames = [\"label\", \"features\"]\n",
        "df       = data.toDF(colNames)\n",
        "\n",
        "# Note, there are lots of cases where you can avoid going from an RDD to a DataFrame.\n",
        "# Perhaps you're importing data from a real database. Or you are using structured streaming\n",
        "# to get your data.\n",
        "# Let's split our data into training data and testing data\n",
        "trainTest   = df.randomSplit([0.5, 0.5])\n",
        "trainingDF  = trainTest[0]\n",
        "testDF      = trainTest[1]\n",
        "\n",
        "# Now create our linear regression model\n",
        "lir   = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
        "\n",
        "# Train the model using our training data\n",
        "model = lir.fit(trainingDF)\n",
        "\n",
        "# Now see if we can predict values in our test data.\n",
        "# Generate predictions using our linear regression model for all features in our\n",
        "# test dataframe:\n",
        "fullPredictions = model.transform(testDF).cache()\n",
        "\n",
        "# Extract the predictions and the \"known\" correct labels.\n",
        "predictions = fullPredictions.select(\"prediction\").rdd.map(lambda x: x[0])\n",
        "labels      = fullPredictions.select(\"label\").rdd.map(lambda x: x[0])\n",
        "\n",
        "# Zip them together\n",
        "predictionAndLabel = predictions.zip(labels).collect()\n",
        "\n",
        "# Stop the session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "cxkr8Ct3q5o7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xbGTktqLvgf1"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}